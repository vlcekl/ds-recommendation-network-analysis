{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquiring data - Web scraping\n",
    "\n",
    "## 1. Download archives with Wikipedia citation records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl = 'https://analytics.wikimedia.org/datasets/archive/public-datasets/all/mwrefs/mwcites-20180301'\n",
    "res = requests.get(baseurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.status_code == requests.codes.ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepage = bs4.BeautifulSoup(res.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_dir = '../data/raw'\n",
    "os.makedirs(target_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and save gzipped tar files from the base page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in basepage.find_all(text=re.compile('gz$')):\n",
    "    #print('file:', fname)\n",
    "    url = baseurl + '/' + fname\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    # save tarbal to arachive\n",
    "    with open(os.path.join(target_dir, fname), 'wb') as fo:\n",
    "        for chunk in res.iter_content(100000):\n",
    "            fo.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untar the archives to target directory (/data/raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "for fname in basepage.find_all(text=re.compile('gz$')):\n",
    "    tar = tarfile.open(os.path.join(target_dir, fname))\n",
    "    tar.extractall(path=target_dir)\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Categorization of Wikipedia pages based on their description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_path = '../data/raw'\n",
    "processed_path = '../data/processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>page_title</th>\n",
       "      <th>rev_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2867096</td>\n",
       "      <td>Mu Aquilae</td>\n",
       "      <td>503137751</td>\n",
       "      <td>2012-07-19 16:08:41</td>\n",
       "      <td>doi</td>\n",
       "      <td>10.1051/0004-6361:20078357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2867096</td>\n",
       "      <td>Mu Aquilae</td>\n",
       "      <td>508363722</td>\n",
       "      <td>2012-08-20 22:56:21</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>astro-ph/0604502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2867096</td>\n",
       "      <td>Mu Aquilae</td>\n",
       "      <td>508363722</td>\n",
       "      <td>2012-08-20 22:56:21</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>astro-ph/0003329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2867096</td>\n",
       "      <td>Mu Aquilae</td>\n",
       "      <td>508363722</td>\n",
       "      <td>2012-08-20 22:56:21</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>0708.1752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2867096</td>\n",
       "      <td>Mu Aquilae</td>\n",
       "      <td>503137751</td>\n",
       "      <td>2012-07-19 16:08:41</td>\n",
       "      <td>doi</td>\n",
       "      <td>10.1051/0004-6361:20064946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_id  page_title     rev_id           timestamp   type  \\\n",
       "0  2867096  Mu Aquilae  503137751 2012-07-19 16:08:41    doi   \n",
       "1  2867096  Mu Aquilae  508363722 2012-08-20 22:56:21  arxiv   \n",
       "2  2867096  Mu Aquilae  508363722 2012-08-20 22:56:21  arxiv   \n",
       "3  2867096  Mu Aquilae  508363722 2012-08-20 22:56:21  arxiv   \n",
       "4  2867096  Mu Aquilae  503137751 2012-07-19 16:08:41    doi   \n",
       "\n",
       "                           id  \n",
       "0  10.1051/0004-6361:20078357  \n",
       "1            astro-ph/0604502  \n",
       "2            astro-ph/0003329  \n",
       "3                   0708.1752  \n",
       "4  10.1051/0004-6361:20064946  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read TSV data\n",
    "df = pd.read_csv(os.path.join(base_path,'enwiki.tsv'), sep='\\t', parse_dates=['timestamp'],infer_datetime_format=True)\n",
    "\n",
    "# Convert mistakenly converted type nan to string 'NaN' (wikipedia page name)\n",
    "df.page_title = df.page_title.fillna(\"NaN\")\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lists of unique pages and publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of unique web page ids (web_page nodes)\n",
    "wp_ids = df.page_id.unique()\n",
    "\n",
    "# list of unique web page names\n",
    "wp_titles = df.page_title.unique()\n",
    "\n",
    "# list of unique publications (publication nodes)\n",
    "pub_ids = df.id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construct the category tree graph**\n",
    "\n",
    "**Implementation:**\n",
    "Perform breadth-first search of the links and add newly found nodes and edges to a NetworkX directed graph.\n",
    "\n",
    "At each step\n",
    "1. Find all categories (nodes) on a page.\n",
    "2. Create links between these and the current page.\n",
    "3. Check if the nodes are already present in the graph\n",
    "4. If not present add to the graph and the to-be-visited list (implemented as collections.deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an empty directed graph\n",
    "G = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "node_list = [(title, {'pid': pid,'address':'/wiki/'+title.replace(' ','_'), 'ptype':'topic'}) for pid, title in zip(wp_ids, wp_titles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create leaf nodes corresponding to Wikipedia pages\n",
    "G.add_nodes_from(node_list[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a queue for pages to be explored in a breadth-first search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# Add all base pages to the queue\n",
    "queue = deque(G.nodes(data=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also define a set of page addresses for quick search of node presence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "node_titles = set([node[0] for node in G.nodes(data=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base URLs for wiki pages and wiki API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_url = 'http://en.wikipedia.org'\n",
    "api_query = 'https://en.wikipedia.org/w/api.php?action=query&format=json&titles='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for scraping wiki pages for categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cats(address):\n",
    "    \"\"\"\n",
    "    Accept a wiki page element and return a list of category elements found on the page.\n",
    "    \"\"\"\n",
    "\n",
    "    r = requests.get(wiki_url+address)\n",
    "    \n",
    "    soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    cats = soup.select('#mw-normal-catlinks ul li a')\n",
    "    \n",
    "    return cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cycle over queue and append new nodes and edges to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 0 1 0 Mu Aquilae topic\n",
      "Iteration: 100 119 220 293 Divination category\n",
      "Iteration: 200 220 421 592 Theories of deduction category\n",
      "Iteration: 300 321 622 938 Aerospace agencies category\n",
      "Iteration: 400 371 772 1260 Form category\n",
      "Iteration: 500 407 908 1595 Signal processing category\n",
      "Iteration: 600 445 1046 1882 International organizations by topic category\n",
      "Iteration: 700 471 1172 2178 Postmodern theory category\n",
      "Iteration: 800 508 1309 2513 Imagination category\n",
      "Iteration: 900 522 1423 2869 Propositions category\n",
      "Iteration: 1000 533 1534 3161 Fields of history category\n",
      "Iteration: 1100 552 1653 3449 Travel category\n",
      "Iteration: 1200 564 1765 3765 Real-time technology category\n",
      "Iteration: 1300 581 1882 4071 Modern physics category\n",
      "Iteration: 1400 590 1991 4389 Ancient Roman religion category\n",
      "Iteration: 1500 626 2127 4733 History of Europe category\n",
      "Iteration: 1600 620 2221 5007 Civil engineering category\n",
      "Iteration: 1700 642 2343 5315 Area studies by ancient history category\n",
      "Iteration: 1800 642 2443 5616 Customary legal systems category\n",
      "Iteration: 1900 632 2533 5959 Physical security category\n",
      "Iteration: 2000 631 2632 6247 Ancient Greek religion category\n",
      "Iteration: 2100 654 2755 6604 Foundations of countries category\n",
      "Iteration: 2200 628 2829 6887 Politics of France category\n",
      "Iteration: 2300 649 2950 7216 Regression models category\n",
      "Iteration: 2400 656 3057 7516 Computer standards category\n",
      "Iteration: 2500 669 3170 7827 Inequality category\n",
      "Iteration: 2600 667 3268 8176 Stock characters category\n",
      "Iteration: 2700 654 3355 8468 World Heritage Sites in Europe category\n",
      "Iteration: 2800 667 3468 8754 History of Europe by topic category\n",
      "Iteration: 2900 643 3544 9022 Chemical process engineering category\n",
      "Iteration: 3000 673 3674 9360 Works about continents category\n",
      "Iteration: 3100 675 3776 9669 Pleistocene category\n",
      "Iteration: 3200 652 3853 9967 Ethically disputed political practices category\n",
      "Iteration: 3300 649 3950 10286 4th-century BC writers category\n",
      "Iteration: 3400 654 4055 10604 Geography of Asia category\n",
      "Iteration: 3500 661 4162 10898 Movements by country category\n",
      "Iteration: 3600 660 4261 11187 Baruch Spinoza category\n",
      "Iteration: 3700 685 4386 11520 Broadcast engineering category\n",
      "Iteration: 3800 692 4493 11793 French writers category\n",
      "Iteration: 3900 697 4598 12130 Languages of Asia category\n",
      "Iteration: 4000 669 4670 12408 Landmarks category\n",
      "Iteration: 4100 633 4734 12700 Types of functions category\n",
      "Iteration: 4200 656 4857 13006 Agrarian politics category\n",
      "Iteration: 4300 638 4939 13308 Water chemistry category\n",
      "Iteration: 4400 643 5044 13630 Dutch computer scientists category\n",
      "Iteration: 4500 628 5129 13889 Modern Latin-language writers category\n",
      "Iteration: 4600 606 5207 14178 Tourism in Italy by city category\n",
      "Iteration: 4700 568 5269 14451 Hunting category\n",
      "Iteration: 4800 569 5370 14807 Réunionnais culture category\n",
      "Iteration: 4900 578 5479 15127 17th century in science category\n",
      "Iteration: 5000 580 5581 15449 19th-century people by occupation and nationality category\n",
      "Iteration: 5100 576 5677 15730 Prehistoric synapsids category\n",
      "Iteration: 5200 544 5745 16016 ISO 15924 category\n",
      "Iteration: 5300 547 5848 16313 Category theory category\n",
      "Iteration: 5400 572 5973 16758 International rivers of Europe category\n",
      "Iteration: 5500 591 6092 17065 Northern Europe category\n",
      "Iteration: 5600 620 6221 17375 Male writers by format category\n",
      "Iteration: 5700 605 6306 17621 Hebrew Bible category\n",
      "Iteration: 5800 600 6401 17957 Chemical compounds by type category\n",
      "Iteration: 5900 622 6523 18253 Hispaniola category\n",
      "Iteration: 6000 681 6682 18567 Transport in Utrecht (province) category\n",
      "Iteration: 6100 691 6792 18864 Organisations based in Spain by city category\n",
      "Iteration: 6200 728 6929 19173 Languages of Iran category\n"
     ]
    }
   ],
   "source": [
    "it = 0\n",
    "while len(queue) > 0:\n",
    "    \n",
    "    # pop the first node in queue\n",
    "    node = queue.popleft()\n",
    "    \n",
    "    # get the address of the page\n",
    "    address = node[1]['address']\n",
    "    \n",
    "    # scrape the page and return tags for categories\n",
    "    cats = get_cats(address)\n",
    "    \n",
    "    # print each 100 cycles\n",
    "    if it%100 == 0:\n",
    "        print('Iteration:', it, len(queue), len(G.nodes()), len(G.edges()), node[0], node[1]['ptype'])\n",
    "    it += 1\n",
    "\n",
    "    # create nodes for the categories (if new)\n",
    "    for c in cats:\n",
    "        \n",
    "        # get category page title (== node identification)\n",
    "        title = c.get_text()\n",
    "        \n",
    "        # create a new edge\n",
    "        G.add_edge(title, node[0])\n",
    "        \n",
    "        if title not in node_titles:\n",
    "            \n",
    "            # get referenced page title\n",
    "            title = c.get_text()\n",
    "            \n",
    "            # get category page address\n",
    "            cat_ref = c.get('href')\n",
    "            \n",
    "            # create a new node tuple\n",
    "            new_node = (title, {'address':cat_ref, 'ptype':'category'})\n",
    "            #print(new_node)\n",
    "            \n",
    "            # create a new node\n",
    "            G.add_node(title, address=cat_ref, ptype='category')\n",
    " \n",
    "            # update queue and node_titles set\n",
    "            queue.append(new_node)\n",
    "            node_titles.add(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternative 1: wikipedia library** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['All articles needing additional references',\n",
       " 'All articles with unsourced statements',\n",
       " 'Aquila (constellation)',\n",
       " 'Articles needing additional references from November 2009',\n",
       " 'Articles with unsourced statements from March 2011',\n",
       " 'Articles with unsourced statements from November 2015',\n",
       " 'Constellations listed by Ptolemy',\n",
       " 'Equatorial constellations',\n",
       " 'Wikipedia articles incorporating a citation from the 1911 Encyclopaedia Britannica with Wikisource reference',\n",
       " 'Wikipedia articles incorporating text from the 1911 Encyclopædia Britannica',\n",
       " 'Wikipedia articles with GND identifiers']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wikipedia\n",
    "wp = wikipedia.page('Aquila_(constellation)')\n",
    "wp.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternative 2: wikipedia API** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_url = 'https://en.wikipedia.org/w/api.php?action=query&format=json&titles='\n",
    "url = category_url + 'Mu_Aquilae'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url)\n",
    "d = r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batchcomplete': '',\n",
      " 'query': {'normalized': [{'from': 'Mu_Aquilae', 'to': 'Mu Aquilae'}],\n",
      "           'pages': {'2867096': {'ns': 0,\n",
      "                                 'pageid': 2867096,\n",
      "                                 'title': 'Mu Aquilae'}}}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither the library of API are helpful for distinguishing topical and maintenance categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
