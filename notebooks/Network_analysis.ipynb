{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read dataset linking wikipedia articles with publications and create a bipartite graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_path = '../data/raw'\n",
    "processed_path = '../data/processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>page_title</th>\n",
       "      <th>rev_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2867096</td>\n",
       "      <td>Mu Aquilae</td>\n",
       "      <td>503137751</td>\n",
       "      <td>2012-07-19 16:08:41</td>\n",
       "      <td>doi</td>\n",
       "      <td>10.1051/0004-6361:20078357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2867096</td>\n",
       "      <td>Mu Aquilae</td>\n",
       "      <td>508363722</td>\n",
       "      <td>2012-08-20 22:56:21</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>astro-ph/0604502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2867096</td>\n",
       "      <td>Mu Aquilae</td>\n",
       "      <td>508363722</td>\n",
       "      <td>2012-08-20 22:56:21</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>astro-ph/0003329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2867096</td>\n",
       "      <td>Mu Aquilae</td>\n",
       "      <td>508363722</td>\n",
       "      <td>2012-08-20 22:56:21</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>0708.1752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2867096</td>\n",
       "      <td>Mu Aquilae</td>\n",
       "      <td>503137751</td>\n",
       "      <td>2012-07-19 16:08:41</td>\n",
       "      <td>doi</td>\n",
       "      <td>10.1051/0004-6361:20064946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_id  page_title     rev_id           timestamp   type  \\\n",
       "0  2867096  Mu Aquilae  503137751 2012-07-19 16:08:41    doi   \n",
       "1  2867096  Mu Aquilae  508363722 2012-08-20 22:56:21  arxiv   \n",
       "2  2867096  Mu Aquilae  508363722 2012-08-20 22:56:21  arxiv   \n",
       "3  2867096  Mu Aquilae  508363722 2012-08-20 22:56:21  arxiv   \n",
       "4  2867096  Mu Aquilae  503137751 2012-07-19 16:08:41    doi   \n",
       "\n",
       "                           id  \n",
       "0  10.1051/0004-6361:20078357  \n",
       "1            astro-ph/0604502  \n",
       "2            astro-ph/0003329  \n",
       "3                   0708.1752  \n",
       "4  10.1051/0004-6361:20064946  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read TSV data\n",
    "df = pd.read_csv(os.path.join(base_path,'enwiki.tsv'), sep='\\t', parse_dates=['timestamp'],infer_datetime_format=True)\n",
    "\n",
    "# Convert mistakenly converted type nan to string 'NaN' (wikipedia page name)\n",
    "df.page_title = df.page_title.fillna(\"NaN\")\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a bipartite graph**\n",
    "\n",
    "Prepare lists of nodes and edges from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of unique web page ids (web_page nodes)\n",
    "wp_ids = df.page_id.unique()\n",
    "\n",
    "# list of unique web page names\n",
    "wp_titles = df.page_title.unique()\n",
    "\n",
    "# list of unique publications (publication nodes)\n",
    "pub_ids = df.id.unique()\n",
    "\n",
    "# list of references (edges)\n",
    "edges = [(page, pub) for page, pub in zip(wp_titles, pub_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wp_node_list = [(title, {'bipartite':'web_page', 'pid': pid,'address':'/wiki/'+title.replace(' ','_'), 'ptype':'topic','depth':0}) for pid, title in zip(wp_ids, wp_titles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pub_node_list = [(pub_id, {'bipartite':'publication'}) for pub_id in pub_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bipartite directed graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(wp_node_list)#, bipartite='web_page')\n",
    "G.add_nodes_from(pub_node_list)#, bipartite='publication')\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make lists of web_page and publication nodes (same as above but, now extracted directly from the graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wpage_nodes = [node for node, data in G.nodes(data=True) if data['bipartite']=='web_page']\n",
    "pub_nodes = [node for node, data in G.nodes(data=True) if data['bipartite']=='publication']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate bipartite degree centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dcent = nx.bipartite.degree_centrality(G, wpage_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign degree of centrality to the two partitions and rank nodeds within each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# webpage ranking\n",
    "wpage_dcent = [(node, dcent[node]) for node in wpage_nodes]\n",
    "wpage_rank = sorted(wpage_dcent, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# publication ranking\n",
    "pub_dcent = [(node, dcent[node]) for node in pub_nodes]\n",
    "pub_rank = sorted(pub_dcent, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create category hierarchy starting at articles based on Wikipedia scraping\n",
    "\n",
    "Add category pages to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a queue for pages to be explored in a breadth-first search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpage_nodes = [(node, data) for node, data in G.nodes(data=True) if data['bipartite']=='web_page']\n",
    "\n",
    "# Add all base pages to the queue\n",
    "queue = deque(wpage_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also define a set of page addresses for quick search of node presence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_titles = set([node for node, data in G.nodes(data=True) if data['bipartite']=='web_page'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base URLs for wiki pages and wiki API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_url = 'http://en.wikipedia.org'\n",
    "api_query = 'https://en.wikipedia.org/w/api.php?action=query&format=json&titles='"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for scraping wiki pages for categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cats(address):\n",
    "    \"\"\"\n",
    "    Accept a wiki page element and return a list of category elements found on the page.\n",
    "    \"\"\"\n",
    "\n",
    "    r = requests.get(wiki_url+address)\n",
    "    \n",
    "    soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    cats = soup.select('#mw-normal-catlinks ul li a')\n",
    "    \n",
    "    return cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cycle over queue and append new nodes and edges to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 857734 3048498 857765 Industrial techno topic\n",
      "Iteration: 100 858218 3049082 858421 William White (architect) topic\n",
      "Iteration: 200 858617 3049581 858998 Cavendish banana topic\n",
      "Iteration: 300 859000 3050064 859595 Lawfare topic\n",
      "Iteration: 400 859360 3050524 860188 Accademia di San Luca topic\n"
     ]
    }
   ],
   "source": [
    "it = 0\n",
    "while len(queue) > 0:\n",
    "    \n",
    "    # pop the first node in queue\n",
    "    node = queue.popleft()\n",
    "    \n",
    "    # get the address of the page\n",
    "    address = node[1]['address']\n",
    "    depth = node[1]['depth']\n",
    "    \n",
    "    # print each 100 cycles\n",
    "    if it%100 == 0:\n",
    "        print('Iteration:', it, len(queue), len(G.nodes()), len(G.edges()), node[0], node[1]['ptype'])\n",
    "    it += 1\n",
    "    \n",
    "    if depth < 3:\n",
    "    \n",
    "        # scrape the page and return tags for categories\n",
    "        cats = get_cats(address)\n",
    "    \n",
    "        # create nodes for the categories (if new)\n",
    "        for c in cats:\n",
    "        \n",
    "            # get category page title (== node identification)\n",
    "            title = c.get_text()\n",
    "        \n",
    "            # create a new edge\n",
    "            G.add_edge(title, node[0])\n",
    "        \n",
    "            if title not in node_titles:\n",
    "            \n",
    "                # get referenced page title\n",
    "                title = c.get_text()\n",
    "            \n",
    "                # get category page address\n",
    "                cat_ref = c.get('href')\n",
    "            \n",
    "                # create a new node tuple\n",
    "                new_depth = depth + 1\n",
    "                new_node = (title, {'address':cat_ref, 'ptype':'category', 'depth':new_depth})\n",
    "                #print(new_node)\n",
    "            \n",
    "                # create a new node\n",
    "                G.add_node(title, address=cat_ref, ptype='category')\n",
    " \n",
    "                # update queue and node_titles set\n",
    "                queue.append(new_node)\n",
    "                node_titles.add(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze the network - degree centrality for page/category subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_subgraph(G, node):\n",
    "    nodes = nx.single_source_shortest_path(G,node).keys()\n",
    "    return G.subgraph(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
