{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publication recommendation system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read dataset linking wikipedia articles with publications and create a bipartite graph of the relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read wikipedia references data from a TSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_path = '../data/raw'\n",
    "processed_path = '../data/processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_id</th>\n",
       "      <th>page_title</th>\n",
       "      <th>rev_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2867096</td>\n",
       "      <td>Mu Aquilae</td>\n",
       "      <td>503137751</td>\n",
       "      <td>2012-07-19 16:08:41</td>\n",
       "      <td>doi</td>\n",
       "      <td>10.1051/0004-6361:20078357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2867096</td>\n",
       "      <td>Mu Aquilae</td>\n",
       "      <td>508363722</td>\n",
       "      <td>2012-08-20 22:56:21</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>astro-ph/0604502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2867096</td>\n",
       "      <td>Mu Aquilae</td>\n",
       "      <td>508363722</td>\n",
       "      <td>2012-08-20 22:56:21</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>astro-ph/0003329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2867096</td>\n",
       "      <td>Mu Aquilae</td>\n",
       "      <td>508363722</td>\n",
       "      <td>2012-08-20 22:56:21</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>0708.1752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2867096</td>\n",
       "      <td>Mu Aquilae</td>\n",
       "      <td>503137751</td>\n",
       "      <td>2012-07-19 16:08:41</td>\n",
       "      <td>doi</td>\n",
       "      <td>10.1051/0004-6361:20064946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_id  page_title     rev_id           timestamp   type  \\\n",
       "0  2867096  Mu Aquilae  503137751 2012-07-19 16:08:41    doi   \n",
       "1  2867096  Mu Aquilae  508363722 2012-08-20 22:56:21  arxiv   \n",
       "2  2867096  Mu Aquilae  508363722 2012-08-20 22:56:21  arxiv   \n",
       "3  2867096  Mu Aquilae  508363722 2012-08-20 22:56:21  arxiv   \n",
       "4  2867096  Mu Aquilae  503137751 2012-07-19 16:08:41    doi   \n",
       "\n",
       "                           id  \n",
       "0  10.1051/0004-6361:20078357  \n",
       "1            astro-ph/0604502  \n",
       "2            astro-ph/0003329  \n",
       "3                   0708.1752  \n",
       "4  10.1051/0004-6361:20064946  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read TSV data\n",
    "df = pd.read_csv(os.path.join(base_path,'enwiki.tsv'), sep='\\t', parse_dates=['timestamp'],infer_datetime_format=True)\n",
    "\n",
    "# Convert mistakenly converted type nan to string 'NaN' (wikipedia page name)\n",
    "df.page_title = df.page_title.fillna(\"NaN\")\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base URLs for wiki pages and wiki API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_url = 'http://en.wikipedia.org'\n",
    "api_query = 'https://en.wikipedia.org/w/api.php?action=query&format=json&titles='\n",
    "cat_tree = 'https://en.wikipedia.org/wiki/Special:CategoryTree'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a bipartite graph connecting wiki pages and publications**\n",
    "\n",
    "Prepare lists of nodes and edges from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of unique web page ids (web_page nodes)\n",
    "wp_ids = df.page_id.unique()\n",
    "\n",
    "# list of unique web page names\n",
    "wp_titles = df.page_title.unique()\n",
    "wp_set = set(wp_titles)\n",
    "\n",
    "# list of unique publications (publication nodes)\n",
    "pub_ids = [(ptype, pid) for ptype, pid in df[['type', 'id']].values]\n",
    "pub_set = set(pub_ids)\n",
    "\n",
    "# list of references (edges)\n",
    "edges = [(page, (ptype, pid)) for page, ptype, pid in df[['page_title', 'type', 'id']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wp_node_list = [(title, {'bipartite':'web_page', 'pid': pid,'address':'/wiki/'+title.replace(' ','_'), 'ptype':'topic','depth':0}) for pid, title in zip(wp_ids, wp_titles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pub_node_list = [(pub_id, {'bipartite':'publication'}) for pub_id in pub_ids] #, 'type':pub_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directed graph (the topic pages and publications form a bipartite graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(wp_node_list)#, bipartite='web_page')\n",
    "G.add_nodes_from(pub_node_list)#, bipartite='publication')\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create category hierarchy starting at articles based on Wikipedia scraping\n",
    "\n",
    "Add category pages to the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to obtain a link and (possibly) title to a publication with a given ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pub_info(pub_id):\n",
    "    \"\"\"\n",
    "    Return the title of the publication with given pub_id\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pub_id: tuple (str, str)\n",
    "            publication id type (doi, isbn, ...) and number\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    link: str\n",
    "          link to a web page with publicatio information\n",
    "    title: str\n",
    "           publication title or 'N/A' if not available\n",
    "    \"\"\"\n",
    "    \n",
    "    # urls for publications with different types of ids\n",
    "    urls = {'doi':'https://doi.org/',\n",
    "            'arxiv':'https://arxiv.org/abs/',\n",
    "            'isbn':'https://isbnsearch.org/isbn/',\n",
    "            'pmid':'https://www.ncbi.nlm.nih.gov/pubmed/',\n",
    "            'pmc':'https://'\n",
    "           }\n",
    "    \n",
    "    link = urls[pub_id[0]] + pub_id[1]\n",
    "\n",
    "    selector = {'doi':'h1',\n",
    "                'arxiv':'h1',# .\"title_mathjax\"',\n",
    "                'isbn':'h1',\n",
    "                'pmid':'h1',\n",
    "                'pmc':'h1'\n",
    "               }\n",
    "\n",
    "    # try to find the title of a publication; if not successful, return N/A.\n",
    "    try:\n",
    "        r = requests.get(link)\n",
    "        try:\n",
    "            soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "            if pub_id[0] == 'arxiv':\n",
    "                title = soup.select(selector[pub_id[0]])[-1].get_text()\n",
    "            else:\n",
    "                title = soup.select(selector[pub_id[0]])[0].get_text()\n",
    "        except:\n",
    "            title = 'N/A'\n",
    "    except requests.exceptions.RequestException:\n",
    "        title = 'N/A'\n",
    "    \n",
    "    return link, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cats(address):\n",
    "    \"\"\"\n",
    "    Accept a wiki page element and return a list of category elements found on the page.\n",
    "    Parameters\n",
    "    ----------\n",
    "    address: str\n",
    "             Address of a wikipedia web page (without the wiki_url base)\n",
    "    Returns\n",
    "    -------\n",
    "    cats: List of BeautifulSoup tag objects\n",
    "          Contains (not-hidden) category information for subsequent processing   \n",
    "    \"\"\"\n",
    "\n",
    "    r = requests.get(wiki_url+address)\n",
    "\n",
    "    soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    cats = soup.select('#mw-normal-catlinks ul li a')\n",
    "\n",
    "    return cats "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to find categories of selected pages and add them to a given graph.\n",
    "Returns the updated graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpage_nodes = [(node, data) for node, data in G.nodes(data=True) if data['bipartite']=='web_page']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set for quickly checking presence of web_page nodes\n",
    "node_titles = set([node for node, data in G.nodes(data=True) if data['bipartite']=='web_page'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_cats(G):\n",
    "\n",
    "    wpage_nodes = [(node, data) for node, data in G.nodes(data=True) if data['bipartite']=='web_page']\n",
    "    \n",
    "    for page in wpage_nodes:\n",
    "\n",
    "        # get the address of the page\n",
    "        address = node[1]['address']\n",
    "\n",
    "        # scrape the page and return tags for categories\n",
    "        cats = get_cats(address)\n",
    "\n",
    "        # create nodes for the categories (if new)\n",
    "        for c in cats:\n",
    "\n",
    "            # get category page title (== node identification)\n",
    "            title = c.get_text()\n",
    "\n",
    "            if title not in node_titles:\n",
    "\n",
    "                # get category page address\n",
    "                cat_ref = c.get('href')\n",
    "\n",
    "                # create a new node tuple\n",
    "                new_node = (title, {'address':cat_ref, 'ptype':'category'})\n",
    "                #print(new_node)\n",
    "\n",
    "                # create a new node\n",
    "                G.add_node(title, address=cat_ref, ptype='category')\n",
    "\n",
    "                # update queue and node_titles set\n",
    "                node_titles.add(title)\n",
    "                \n",
    "            # create a new edge pointing from higher level category to the present page\n",
    "            G.add_edge(title, node[0])\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go over catogory nodes and find derived topic articles from the catogory tree form.\n",
    "Use web scraping of the form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cat_children(cat_name):\n",
    "    \"\"\"\n",
    "    Obtains wikipedia articles that are children of a given category\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cat_name: str\n",
    "              Category name\n",
    "              \n",
    "    Returns\n",
    "    -------\n",
    "    article_list: list of Beautiful tag objects\n",
    "    \"\"\"\n",
    "    \n",
    "    cat_tree = 'https://en.wikipedia.org/wiki/Special:CategoryTree'\n",
    "    cat_name = cat_name.replace(' ','+')\n",
    "    query = '?target=' + cat_name + '&mode=all&namespaces=0&title=Special%3ACategoryTree'\n",
    "    \n",
    "    r = requests.get(cat_tree+query)\n",
    "    \n",
    "    soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    article_list = soup.find_all('a', {'class':'CategoryTreeLabelPage'})\n",
    "    \n",
    "    return article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = get_cat_tree('Quantum Monte Carlo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum Monte Carlo\n",
      "/wiki/Quantum_Monte_Carlo \n",
      "\n",
      "Auxiliary-field Monte Carlo\n",
      "/wiki/Auxiliary-field_Monte_Carlo \n",
      "\n",
      "CASINO\n",
      "/wiki/CASINO \n",
      "\n",
      "Diffusion Monte Carlo\n",
      "/wiki/Diffusion_Monte_Carlo \n",
      "\n",
      "Gaussian quantum Monte Carlo\n",
      "/wiki/Gaussian_quantum_Monte_Carlo \n",
      "\n",
      "Path integral molecular dynamics\n",
      "/wiki/Path_integral_molecular_dynamics \n",
      "\n",
      "Path integral Monte Carlo\n",
      "/wiki/Path_integral_Monte_Carlo \n",
      "\n",
      "Reptation Monte Carlo\n",
      "/wiki/Reptation_Monte_Carlo \n",
      "\n",
      "Time-dependent variational Monte Carlo\n",
      "/wiki/Time-dependent_variational_Monte_Carlo \n",
      "\n",
      "Variational Monte Carlo\n",
      "/wiki/Variational_Monte_Carlo \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for article in article_list:\n",
    "    print(article.get_text())\n",
    "    print(article.get('href'),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to rank publications in a given subgraph by their bipartite degree centrality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank_publications(G):\n",
    "    \"\"\"\n",
    "    Calcualate bipartite degree centrality ranking of publications within a given graph G.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    G: networkx graph object\n",
    "       usually a subgraph of the main graph containing publication nodes (bipartite=publication)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pub_rank: list of tuples (graph node, degree centrality) ordered by degree centrality\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select publication and topic page nodes\n",
    "    pub_nodes = [node for node, data in G.nodes(data=True) if data['bipartite'] == 'publication']\n",
    "    wpage_nodes = [node for node, data in G.nodes(data=True) if data['bipartite'] == 'web_page']\n",
    "\n",
    "    # Create a bipartite subgraph of topic nodes and publications\n",
    "    SG = G.subgraph(pub_nodes + wpage_nodes)\n",
    "\n",
    "    # calcualate degree centrality\n",
    "    dcent = nx.bipartite.degree_centrality(SG, wpage_nodes)\n",
    "    \n",
    "    # save webpage nodes with their degree centrality\n",
    "    pub_dcent = [(node, dcent[node]) for node in pub_nodes]\n",
    "    \n",
    "    # sort publication nodes from highest to lowest\n",
    "    pub_rank = sorted(pub_dcent, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return pub_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to find N most highle referenced publications related to the original publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def find_most_relevant(G, publication, n_highest_pubs=10):\n",
    "    \"\"\"\n",
    "    Finds and prints N most relevant publications to the original publication (based on citations)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    publication: tuple (str, str)\n",
    "                 Identification of the original publication based on id type and number\n",
    "    n_highest_pubs: int\n",
    "                    number of publications to list\n",
    "                    \n",
    "    Returns\n",
    "    -------\n",
    "    Prints list on the screen\n",
    "    \"\"\"\n",
    "    \n",
    "    # Original publication and its wikipedia citations\n",
    "    print('Original publication:', publication, '\\nTitle:', pub_info(publication)[1],'\\n\\n')\n",
    "    page_list = G.predecessors(publication)\n",
    "    print(len(page_list), 'pages referring to the publication:\\n', page_list,'\\n\\n')\n",
    "    \n",
    "    # Level 1 - citing topic pages\n",
    "    # Find all publications referenced by wiki pages that also cite the original publication\n",
    "    infos = []\n",
    "    for page in page_list:\n",
    "        infos.extend([pub for pub in G.successors(page)])\n",
    "    \n",
    "    # create a subgraph of related pages and publications\n",
    "    SG = G.subgraph(page_list + infos)\n",
    "    \n",
    "    # Add category nodes and descending pages\n",
    "    add_cats(SG)\n",
    "        \n",
    "    # Level 2 - Categories from citing topic pages\n",
    "    # Find pages linked to the topic pages through common category\n",
    "    # Get additional publications through their descendants (exclude those in level 1)\n",
    "    for page in page_list:\n",
    "        # Add category nodes to the graph\n",
    "        cats = get_cats(page)\n",
    "    \n",
    "    # Select N most highly cited publications\n",
    "    pub_counts = Counter(infos)\n",
    "    del pub_counts[publication]\n",
    "    most_cited = pub_counts.most_common(n_highest_pubs)\n",
    "    \n",
    "    # Print information about the highly cited publications\n",
    "    for i, pub in enumerate(most_cited, 1):\n",
    "        info = pub_info(pub[0])\n",
    "        print('Rank:', i, '\\nCitations:', pub[1])\n",
    "        print('ID:', pub[0])\n",
    "        print('Source:', info[0])\n",
    "        print('Title:', info[1],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Tests of the recommendation system**\n",
    "\n",
    "* so far only links through topic wikipedia pages\n",
    "* todo: add catogory information - important for publications with fewer linking wikipedia pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original publication: ('arxiv', '1208.3048') \n",
      "Title: Title:\n",
      "Radial velocities for the Hipparcos-Gaia Hundred-Thousand-Proper-Motion  project \n",
      "\n",
      "\n",
      "243 pages referring to the publication:\n",
      " ['8 Cancri', 'Mu1 Cancri', '12 Cancri', '28 Cancri', 'Beta Sextantis', 'Kappa Canis Majoris', 'Nu3 Canis Majoris', 'Xi2 Canis Majoris', 'Gamma Canis Minoris', 'Delta1 Canis Minoris', 'Delta3 Canis Minoris', 'Epsilon Canis Minoris', 'Zeta Canis Minoris', 'Nu Arietis', 'Omicron Arietis', 'Upsilon Tauri', 'Sigma Tauri', 'Pi Tauri', 'Psi Tauri', 'Psi Velorum', 'Upsilon2 Eridani', 'Omega Herculis', 'Theta Cassiopeiae', 'Epsilon Trianguli Australis', 'Kappa Trianguli Australis', 'Iota Trianguli Australis', 'Theta Trianguli Australis', 'Eta Trianguli Australis', 'Pi Tucanae', 'Lambda1 Tucanae', 'Lambda2 Tucanae', 'Upsilon Ursae Majoris', 'Kappa Volantis', 'Omega Ursae Majoris', 'Tau Ursae Majoris', 'Rho Ursae Majoris', '44 Andromedae', 'HR 515', 'Mu Aurigae', 'Xi Aurigae', '45 Aurigae', 'Delta Ursae Minoris', 'Omicron Aurigae', '39 Aurigae', '42 Aurigae', 'Theta Ursae Minoris', 'Nu1 Boötis', 'Nu2 Boötis', 'Chi Boötis', 'Pi1 Ursae Minoris', 'Theta Cancri', 'Lambda Cancri', 'Mu2 Cancri', 'Nu Cancri', 'Xi Cancri', 'Omicron1 Cancri', 'Mu Serpentis', 'Xi Serpentis', 'Epsilon Serpentis', '82 Cancri', 'Rho2 Cancri', 'Sigma1 Cancri', 'Sigma2 Cancri', 'Upsilon1 Cancri', 'Upsilon2 Cancri', 'Phi1 Cancri', 'Omicron Serpentis', 'Pi Serpentis', 'Sigma Serpentis', 'Chi Serpentis', 'Upsilon Serpentis', 'Iota1 Scorpii', 'Epsilon Cancri', 'Kappa1 Sculptoris', 'Kappa2 Sculptoris', 'Sigma Sculptoris', 'Xi Sculptoris', 'Tau Sculptoris', 'Lambda1 Sculptoris', 'Tau Centauri', 'Upsilon2 Centauri', '1 Centauri', 'Xi1 Centauri', 'Delta Caeli', 'Nu Caeli', 'Zeta Caeli', 'Gamma Camelopardalis', 'Alpha Horologii', 'Psi3 Aurigae', 'Psi6 Aurigae', 'Delta Ceti', 'Theta Ceti', 'Upsilon Ceti', 'Beta Crateris', 'Upsilon Librae', 'Delta Hydrae', '14 Eridani', 'Sigma Columbae', 'Pi1 Columbae', 'Pi2 Columbae', 'Delta Doradus', 'Xi Cassiopeiae', 'Nu Cassiopeiae', 'Pi Cassiopeiae', 'Omega Cassiopeiae', 'Upsilon Piscium', 'Theta Doradus', 'Lambda Muscae', 'Beta Horologii', 'Xi Hydrae', '1 Scorpii', 'Sigma1 Ursae Majoris', 'Epsilon Herculis', 'Zeta Mensae', 'Mu Mensae', 'Epsilon Doradus', 'Delta Horologii', 'Upsilon2 Hydrae', 'Rho Ceti', 'Sigma Ceti', 'Chi Ceti', 'Beta Reticuli', 'Gamma Reticuli', 'Delta Reticuli', 'Eta Reticuli', 'Iota Reticuli', 'Kappa Reticuli', 'Phi3 Hydrae', 'Chi1 Hydrae', 'Gamma Crateris', 'Psi Hydrae', 'Omega Hydrae', 'Tau1 Hydrae', 'Tau2 Hydrae', 'Phi1 Hydrae', 'Phi2 Hydrae', 'Iota Coronae Borealis', 'Zeta Crateris', 'Epsilon Crateris', 'Kappa Leporis', 'Phi Eridani', 'Upsilon3 Eridani', 'Iota Eridani', 'Gamma Hydri', 'Kappa Eridani', 'Omega Eridani', 'Tau9 Eridani', '13 Comae Berenices', '31 Leonis', '64 Eridani', '16 Librae', 'Kappa Librae', 'Delta Pyxidis', 'Epsilon Pyxidis', 'Eta Pyxidis', 'Theta Pyxidis', 'Kappa Pyxidis', 'Tau Leonis', 'Zeta Lupi', 'Eta Lupi', 'Iota Lupi', 'Phi1 Lupi', 'Kappa1 Lupi', 'Rho Lupi', 'Eta Crateris', 'Gamma Circini', 'Xi Orionis', 'Epsilon Circini', 'Zeta Circini', 'Mu Canis Majoris', 'Lambda Lupi', 'Theta Lupi', 'Mu Lupi', 'Omicron Lupi', 'Tau2 Lupi', 'Omega Lupi', 'Sigma Lupi', 'Phi2 Lupi', 'Psi2 Lupi', 'Eta Horologii', 'Mu Horologii', 'Gamma Mensae', '75 Tauri', 'Mu Coronae Borealis', 'Chi Piscium', 'Rho Piscium', 'Sigma Herculis', 'Phi Herculis', 'Chi Herculis', 'Psi2 Piscium', 'Psi3 Piscium', 'Mu Muscae', 'Nu1 Coronae Borealis', 'Nu2 Coronae Borealis', 'Kappa Pictoris', 'Tau7 Eridani', 'Epsilon Normae', 'Iota1 Normae', 'Eta Normae', 'Delta Normae', 'Kappa Normae', 'Gamma1 Normae', 'Eta1 Hydri', 'Theta Normae', 'Nu Horologii', 'Xi1 Lupi', 'Xi2 Lupi', 'Upsilon Lupi', 'Lambda Horologii', 'Delta Mensae', 'Lambda Librae', '38 Leonis Minoris', 'Omega Geminorum', 'Iota1 Muscae', 'Phi3 Ceti', 'Nu Leporis', 'Kappa Hydrae', 'Chi2 Hydrae', '57 Persei', 'Epsilon Fornacis', 'Iota Crateris', 'Kappa Crateris', 'Zeta Normae', 'Eta2 Pictoris', 'Eta1 Pictoris', 'Mu Pictoris', 'Psi Crateris', 'Upsilon Coronae Borealis', 'Iota Pictoris', 'Zeta Fornacis', 'Iota2 Fornacis', 'Mu Fornacis', 'Zeta Pictoris', '64 Piscium', 'Theta Hydri', 'Mu Librae', 'HD 76653', 'HD 2454', 'HD 111456', 'Phi Phoenicis', 'Xi Phoenicis', 'Gamma Horologii', 'Omega2 Tauri'] \n",
      "\n",
      "\n",
      "Rank: 1 \n",
      "Citations: 243\n",
      "ID: ('doi', '10.1051/0004-6361/201219219')\n",
      "Source: https://doi.org/10.1051/0004-6361/201219219\n",
      "Title: N/A \n",
      "\n",
      "Rank: 2 \n",
      "Citations: 241\n",
      "ID: ('doi', '10.1051/0004-6361:20078357')\n",
      "Source: https://doi.org/10.1051/0004-6361:20078357\n",
      "Title: N/A \n",
      "\n",
      "Rank: 3 \n",
      "Citations: 241\n",
      "ID: ('arxiv', '0708.1752')\n",
      "Source: https://arxiv.org/abs/0708.1752\n",
      "Title: Title:\n",
      "Validation of the new Hipparcos reduction \n",
      "\n",
      "Rank: 4 \n",
      "Citations: 140\n",
      "ID: ('arxiv', '0806.2878')\n",
      "Source: https://arxiv.org/abs/0806.2878\n",
      "Title: Title:\n",
      "A Catalog of Multiplicity among Bright Stellar Systems \n",
      "\n",
      "Rank: 5 \n",
      "Citations: 140\n",
      "ID: ('doi', '10.1111/j.1365-2966.2008.13596.x')\n",
      "Source: https://doi.org/10.1111/j.1365-2966.2008.13596.x\n",
      "Title: N/A \n",
      "\n",
      "Rank: 6 \n",
      "Citations: 136\n",
      "ID: ('doi', '10.1134/S1063773712050015')\n",
      "Source: https://doi.org/10.1134/S1063773712050015\n",
      "Title: XHIP: An extended hipparcos compilation \n",
      "\n",
      "Rank: 7 \n",
      "Citations: 136\n",
      "ID: ('arxiv', '1108.4971')\n",
      "Source: https://arxiv.org/abs/1108.4971\n",
      "Title: Title:\n",
      "XHIP: An Extended Hipparcos Compilation \n",
      "\n",
      "Rank: 8 \n",
      "Citations: 119\n",
      "ID: ('arxiv', '1208.2037')\n",
      "Source: https://arxiv.org/abs/1208.2037\n",
      "Title: Title:\n",
      "Fundamental parameters and infrared excesses of Hipparcos stars \n",
      "\n",
      "Rank: 9 \n",
      "Citations: 119\n",
      "ID: ('doi', '10.1111/j.1365-2966.2012.21873.x')\n",
      "Source: https://doi.org/10.1111/j.1365-2966.2012.21873.x\n",
      "Title: N/A \n",
      "\n",
      "Rank: 10 \n",
      "Citations: 67\n",
      "ID: ('arxiv', '1501.03154')\n",
      "Source: https://arxiv.org/abs/1501.03154\n",
      "Title: Title:\n",
      "The Ages of Early-Type Stars: Strömgren Photometric Methods  Calibrated, Validated, Tested, and Applied to Hosts and Prospective Hosts of  Directly Imaged Exoplanets \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 1: a random astronomical publication\n",
    "find_most_relevant(('arxiv', '1208.3048'), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original publication: ('doi', '10.1021/j100308a038') \n",
      "Title: The missing term in effective pair potentials \n",
      "\n",
      "\n",
      "2 pages referring to the publication:\n",
      " ['Water model', 'Solvent models'] \n",
      "\n",
      "\n",
      "Rank: 1 \n",
      "Citations: 2\n",
      "ID: ('pmid', '25660403')\n",
      "Source: https://www.ncbi.nlm.nih.gov/pubmed/25660403\n",
      "Title: PubMed \n",
      "\n",
      "Rank: 2 \n",
      "Citations: 2\n",
      "ID: ('doi', '10.1039/C5CP00288E')\n",
      "Source: https://doi.org/10.1039/C5CP00288E\n",
      "Title: N/A \n",
      "\n",
      "Rank: 3 \n",
      "Citations: 1\n",
      "ID: ('doi', '10.1021/jp973084f')\n",
      "Source: https://doi.org/10.1021/jp973084f\n",
      "Title: All-Atom Empirical Potential for Molecular Modeling and Dynamics Studies of Proteins †   \n",
      "\n",
      "Rank: 4 \n",
      "Citations: 1\n",
      "ID: ('doi', '10.1063/1.2038787')\n",
      "Source: https://doi.org/10.1063/1.2038787\n",
      "Title: N/A \n",
      "\n",
      "Rank: 5 \n",
      "Citations: 1\n",
      "ID: ('doi', '10.1063/1.2360276')\n",
      "Source: https://doi.org/10.1063/1.2360276\n",
      "Title: N/A \n",
      "\n",
      "Rank: 6 \n",
      "Citations: 1\n",
      "ID: ('doi', '10.1021/jz501780a')\n",
      "Source: https://doi.org/10.1021/jz501780a\n",
      "Title: Building Water Models: A Different Approach \n",
      "\n",
      "Rank: 7 \n",
      "Citations: 1\n",
      "ID: ('doi', '10.1021/jp046158d')\n",
      "Source: https://doi.org/10.1021/jp046158d\n",
      "Title: Temperature Dependence of Water Vibrational Spectrum:  A Molecular Dynamics Simulation Study \n",
      "\n",
      "Rank: 8 \n",
      "Citations: 1\n",
      "ID: ('doi', '10.1016/j.cplett.2012.05.044')\n",
      "Source: https://doi.org/10.1016/j.cplett.2012.05.044\n",
      "Title: N/A \n",
      "\n",
      "Rank: 9 \n",
      "Citations: 1\n",
      "ID: ('doi', '10.1063/1.444325')\n",
      "Source: https://doi.org/10.1063/1.444325\n",
      "Title: N/A \n",
      "\n",
      "Rank: 10 \n",
      "Citations: 1\n",
      "ID: ('doi', '10.1063/1.1931662')\n",
      "Source: https://doi.org/10.1063/1.1931662\n",
      "Title: N/A \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Publication introducing a popular molecular model\n",
    "find_most_relevant(('doi', '10.1021/j100308a038'), 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
